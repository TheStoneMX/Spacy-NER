{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b7842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import random\n",
    "import logging\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad80f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572bfc05",
   "metadata": {},
   "source": [
    "### convert_data_to_spacy \n",
    "takes a JSON file path as an input and returns the training data formatted in a way that is suitable for training a spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae08042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert data \n",
    "def convert_data_to_spacy(JSON_FilePath):\n",
    "    try:\n",
    "        ''' \n",
    "        Initialize empty list training_data to store the training data.\n",
    "        Initialize empty list lines to store the lines from the input file.'''\n",
    "        training_data = [] \n",
    "        lines=[]\n",
    "        '''\n",
    "        Open the input file using a with statement, specifying the file path, read mode ('r'), \n",
    "        and UTF-8 encoding.\n",
    "        '''\n",
    "        with open(JSON_FilePath, 'r',encoding='utf-8') as f: # open the json file\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Iterate through each line in lines.\n",
    "        for line in lines:\n",
    "            \n",
    "            data = json.loads(line) #Load the JSON data and store it in the data variable\n",
    "            text = data['content'] # Extract the text content from the data dictionary and store it in the text variable.\n",
    "            entities = [] # Initialize an empty list entities to store entity annotations\n",
    "            \n",
    "            #Iterate through each annotation in data['annotation']\n",
    "            for annotation in data['annotation']:\n",
    "                #only a single point in text annotation.\n",
    "                point = annotation['points'][0] # Get the starting and ending points of the entity from the points key\n",
    "                labels = annotation['label'] # Extract the entity label(s) from the label key in the annotation dictionary.\n",
    "                \n",
    "                # handle both list of labels or a single label.\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "                \n",
    "                # Iterate through each label in the labels list and append a tuple (start, end, label) to the entities list.\n",
    "                for label in labels:\n",
    "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "            # Append a tuple (text, {\"entities\": entities}) to the training_data list.\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "        # After processing all lines, return the training_data list.\n",
    "        return training_data\n",
    "\n",
    "    # If any exception occurs during the process, log the exception with a message and return None.\n",
    "    except Exception as e: # in case of exception print-\n",
    "        logging.exception(\"Unable to process \" + JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35aa0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = convert_data_to_spacy(\"./input/training/Entity Recognition in Resumes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b08e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check_existing_model takes a model name as input and checks if the model exists or not. \n",
    "If the model exists, it prints \"Model Exists. Updating the model\" and returns the model name. \n",
    "If the model does not exist, it prints \"Model by this name does not exist. \n",
    "Building a new one\" and returns None.'''\n",
    "\n",
    "def check_existing_model(model_name): # take model name as an input\n",
    "# pass this in a try except block\n",
    "    try: \n",
    "        nlp=spacy.load(model_name)\n",
    "        print(\"Model Exists. Updating the model\")\n",
    "        return model_name\n",
    "    except Exception as e: # exception\n",
    "        print(\"Model by this name does not exist. Building a new one\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5494941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model by this name does not exist. Building a new one\n"
     ]
    }
   ],
   "source": [
    "model = check_existing_model(\"nlp_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6fd5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def build_spacy_model(train_data,model):\n",
    "\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    TRAIN_DATA = train_data\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")     \n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        if model is None:\n",
    "            optimizer = nlp.begin_training()\n",
    "        for itn in range(2):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            # random.shuffle(TRAIN_DATA)\n",
    "            # losses = {}\n",
    "            # batches = minibatch(TRAIN_DATA, size=compounding(8., 32., 1.001))\n",
    "            # for batch in batches:\n",
    "            #     texts, annotations = zip(*batch)\n",
    "            #     nlp.update(texts, annotations, sgd=optimizer, \n",
    "            #                losses=losses)\n",
    "            # print('Losses', losses)\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                    try:\n",
    "                        nlp.update(\n",
    "                            [text],  # batch of texts\n",
    "                            [annotations],  # batch of annotations\n",
    "                            drop=0.2,  # dropout - make it harder to memorise data\n",
    "                            sgd=optimizer,  # callable to update weights\n",
    "                            losses=losses)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "            print(losses)\n",
    "    \n",
    "    nlp.to_disk(\"model\")\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "355c0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_spacy_model(train_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7a3dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "\n",
    "function for text convertion \n",
    "def convert_pdf_to_text(dir):\n",
    "    output=[]\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        print(files)\n",
    "        for file in files:\n",
    "            path_to_pdf = os.path.join(root, file)\n",
    "            #print(path_to_pdf)\n",
    "            [stem, ext] = os.path.splitext(path_to_pdf)\n",
    "            if ext == '.pdf':\n",
    "                print(\"Processing \" + path_to_pdf)\n",
    "                pdf_contents = parser.from_file(path_to_pdf,service='text')\n",
    "                path_to_txt = stem + '.txt'\n",
    "                # with open(path_to_txt, 'w',encoding='utf-8') as txt_file:\n",
    "                #     print(\"Writing contents to \" + path_to_txt)\n",
    "                #     txt_file.write(pdf_contents['content'])\n",
    "                output.append(pdf_contents['content'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151989f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a91844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0782f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8c07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29e724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187fd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c008ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (NLP)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
